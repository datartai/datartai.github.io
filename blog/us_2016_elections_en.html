<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no"/>
    <meta name="theme-color" content="#2980BA">
    <title>Datart</title>

    <!-- CSS  -->
    <link href="../min/plugin-min.css" type="text/css" rel="stylesheet">
    <link href="../min/custom-min.css" type="text/css" rel="stylesheet" >

    <!-- JS  -->
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
</head>

<body id="top" class="scrollspy">

<!-- Pre Loader -->
<div id="loader-wrapper">
    <div id="loader"></div>
    <div class="loader-section section-left"></div>
    <div class="loader-section section-right"></div>
</div>

<!--Navigation-->
<div class="navbar-fixed">
    <nav id="nav_f" class="default_color" role="navigation">
        <div class="container">
            <div class="nav-wrapper">
            <a href="#" id="logo-container"><img style="padding-top: 10px;" width="15%" src="../img/datart_branco.svg"></a>
                <ul class="right hide-on-med-and-down">
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="#">Blog</a></li>
                </ul>
                <ul id="nav-mobile" class="side-nav">
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="#">Blog</a></li>
                </ul>
            <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="mdi-navigation-menu"></i></a>
            </div>
        </div>
    </nav>
</div>

<!-- Blog posts -->
<div id="intro" class="section scrollspy">
<div class="container">
<div class="row">
<div class="col s12">

<h2 class="center header text_h2">How (<b>not</b>) to forecast an election</h2>

<center><span class="span_h2">We use a hierarchical Bayesian model to show how a simple pro-Trump bias has a huge effect on the forecasting results.</span></center>
<br>

<h4 class="text_h4">Introduction</h4>

<p>Unlike other countries, where presidential elections follow a simple majority rule, the US election follows a different set of rules that makes it very hard for outsiders to understand what is going on. However, the idea is quite simple: each state has a number of electoral college votes, totalling 538 votes across the nation. The winner candidate of each state election takes all its electoral college votes. If a candidate receives 270 electoral votes or more overall, he or she wins the election. This makes the US election an interesting forecasting problem. Instead of predicting the total number of votes in each candidate, the problem is forecasting the result of each state individually.</p>


"Prediction is very difficult, especially if it's about the future." Niels Bohr (DISPUTED)


“An economist is an expert who will know tomorrow why the things he predicted yesterday didn't happen today.” Laurence Peter



<p>Our interest is not to forecast the election, which is not a random variable anymore (spoiler alert: Trump won). We want to show how a simple Bayesian forecasting model works, and propose a modification that explains why statistical models would fail so badly in this election if there were polling biases.</p>


<h4 class="text_h4">Polling</h4>

<p>In theory, a poll should randomly sample the voting population (the sampling is often stratified). However, in practice there are several possible polling problems that can compromise their use in naive statistical analyses:</p>
<ul class="collection">
    <li class="collection-item"><b><a href="https://en.wikipedia.org/wiki/Shy_Tory_Factor">Shy tory</a></b> / <b><a href="https://en.wikipedia.org/wiki/Bradley_effect">Bradley effect</a></b> / <b>“shame” effect</b>: Voters are not comfortable with revealing their particular preference</li>
    <li class="collection-item"><b>Selection bias</b>: People who answer the pollsters are from a particular group (e.g. people who usually are at home are different in general from people who usually aren’t)</li>
    <li class="collection-item"><b>Stratification errors</b>: Wrong subpopulation determination.</li>
    <li class="collection-item"><b>Non-voting correlation with a candidate</b>: As voting is not mandatory, the presidential preference of some people may be correlated with their chance of (not) voting</li>
    <li class="collection-item"><b>Temporal preference change</b>: The preferences change over time -- a lot of people make up their minds in the last week</li>
    <li class="collection-item"><b>Sampling error</b>: Any sample from the population will be noisy and subject to sampling errors.</li>
</ul>


<p>Simple forecasts usually only consider the last item when estimating the margins of error or uncertainty. If you only consider this and nothing more, multiple polls will be treated as independent, and the overall error will reduce until there is absolute certainty (which is never the case). The other effects are not directly available for modeling, so the only way to estimate them is by making assumptions or using polling results from previous elections. We do not attempt to model exactly those issues here, we rather include all of them in a bias term that is shared across all polls, and show that even a small bias term favorable to Trump completely changes the forecast.</p>

<h4 class="text_h4">Forecast</h4>

<p>We leave the details of our hierarchical Bayesian forecasting model at the end, for the advanced reader. Now we show the results of its forecasts. However, depending on which polls we include, we have very different results. It is important that the poll is recent and of good quality (appropriate methodology, history, etc). As a proxy of quality, we use the letter grades from 538.</p>


<p>We found that the simple choice of which polls to use has a very large impact on the probability of victory of each candidate:</p>
<div class="card">
<div class="card-action">
<table class="striped" >
<thead>
  <tr>
      <th data-field="id">Polling choice</th>
      <th data-field="name">Clinton (%)</th>
      <th data-field="price">Trump (%)</th>
      <th data-field="price">Others (%)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Last with good grade</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Best 3 grades over the last month</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Last 3 with good grade</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>All last week</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</tbody>
</table>
</div>
</div>


<p>By aggregating the polls with different weights (e.g. according to their quality or distance from election), we would have endless forecasting possibilities, which explains the diversity found in the media before the election:
NY Times: 85% Clinton, 15% Trump http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html?_r=0
Huff Post: 98% Clinton, 1.7% Trump http://elections.huffingtonpost.com/2016/forecast/president
Princeton Election Consortium: 93% Clinton http://election.princeton.edu/2016/11/08/final-mode-projections-clinton-323-ev-51-di-senate-seats-gop-house/
538: Clinton at 71.4% and Trump at 28.6% http://projects.fivethirtyeight.com/2016-election-forecast/)</p>


<p>Of the mainstream forecasters, only 538 cannot be included in the same bucket of almost-certain Clinton victory. Notice that the other forecasts are consistent with our own results. How come so many forecasters made such egregious mistakes? As in many statistical problems, the answer lies with the data: garbage in, garbage out.</p>


<h4 class="text_h4">Bias impact</h4>

<b>Explicar bias aqui</b>

<img src="../img/blog/bias_influence.png">


We see that even a small polling bias that favors Clinton has a huge effect on the election forecast. This explains why 538 had a more favorable Trump forecast as they include a polling bias and do not treat the polls as independent samples, but this also indicates that even 538 underestimated the polling biases.


You can check how the bias impacts the result for yourself:


D3 MAP here


<h4 class="text_h4">Hierarchical Bayesian forecasting model</h4>


<center><img src="../img/blog/elections_graphical_model.svg" width="60%"></center>

<p>How do you aggregate the information of each state poll in a globally coherent prediction? We can use a hierarchical Bayesian model. Overall, each poll’s likelihood is modelled as a multinomial, with dirichlet prior (per state) and normal hyper-prior (per nation). This way, prior information is shared across states and we can use weak hyper-priors. We start with an overall national preference over each candidate, modeled as three independent wide uniform distributions:</p>

$$ \alpha_c \sim Uniform(0, 1000), c \in {Trump, Clinton, Independents} $$

<p>Then, we have the voting intention in each state, with the national preference as prior:</p>
$$ \theta_s \sim Dirichlet(\alpha), s \in States $$
<p>Finally, each poll is modeled as one independent sample of the voting intention of the state:</p>
$$ poll_{s,d} \sim Multinomial(\theta_s, N_{poll_{s,d}}), s \in States, d \in Dates $$


<p>Bayesian inference of the posteriors</p>


<p>To forecast the probability of each candidate winning the state election, we use the posterior of the voting intentions to sample the election result of the state. Thus, for each state we sample the predicted number of votes of each candidate in election day using the following formula:</p>

$$ election_s \sim Multinomial(\theta_s, N_{voters_s}), s \in States $$

<p>The candidate with more votes takes all the electoral colleges of the state (we ignore the particularities of Nebraska and Maine). We sum the electoral colleges votes of each candidates, and if a candidate wins 270 votes or more, he or she is the winner. We repeat this process multiple times in order to determine the probability of each candidate winning the election.</p>


<p>We sample the election result in all states multiple times, and for each time we calculate the number of electoral colleges won by each candidate, and the corresponding victor. We implemented the model in PyStan.</p>


To test how much our model is robust to polling errors, we add a small bias term in the election prediciton:
$$ \theta^{bias}_s \leftarrow [\theta_{s_{Clinton}}  - bias, \theta_{s_{Trump}}  + bias, theta_{s_{Ind}}] \\
bias \sim Uniform(0, \epsilon) \\
election_s \sim Multinomial(\theta^{bias}_s, N^{voters}_s) \\
s \in States $$


We vary \( \epsilon \) from 0 to 3% and see how it effects our election forecast:


<h4 class="text_h4">Code</h4>

Stan code -- nothing more than that -- link the github notebook here

<xmp>
data {
    int nb_polls;
    int nb_states;
    int nb_candidates;
    int polls_states[nb_polls];
    int votes[nb_polls, nb_candidates];
    int nb_voters[nb_states];
    real bias;
}
parameters {
    simplex[nb_candidates] theta[nb_states]; //1 - trump, 2 - clinton, 3 - independent
    vector[nb_candidates] alpha;
}
model {
    for(c in 1:nb_candidates)
        alpha[c] ~ uniform(0, 1000);

    for(s in 1:nb_states)
        theta[s] ~ dirichlet(alpha);

    for(p in 1:nb_polls)
        votes[p] ~ multinomial(theta[polls_states[p]]);
}
generated quantities {
    int votes_pred[nb_states, nb_candidates];
    real epsilon[nb_states];
    simplex[nb_candidates] theta_bias[nb_states];
    real delta_t[nb_states];
    real delta_h[nb_states];
    real delta[nb_states];

    for(s in 1:nb_states) {
        epsilon[s] <- uniform_rng(0, bias);
        delta_t[s] <- fabs(theta[s][1] - fmax(0, fmin(1, theta[s][1] + epsilon[s])));
        delta_h[s] <- fabs(theta[s][2] - fmin(1, fmax(0, theta[s][2] - epsilon[s])));
        //delta[s] <- (epsilon[s]/fabs(epsilon[s]))*fmin(delta_t[s], delta_h[s]);
        delta[s] <- fmin(delta_t[s], delta_h[s]);
        theta_bias[s][1] <- theta[s][1] + delta[s];
        theta_bias[s][2] <- theta[s][2] - delta[s];
        theta_bias[s][3] <- theta[s][3];
        votes_pred[s] <- multinomial_rng(theta_bias[s], nb_voters[s]);
    }
}
</xmp>

<h4 class="text_h4">Conclusion</h4>


Conlusão mais geral, porque o parágrafo seguinte são os próximos passos:


<p>Nassim Taleb said before the election that the forecasts were not reliable due to their significant volatility (which according to his model should have pulled the probabilities toward 50-50)<a href="https://mishtalk.com/2016/08/07/nassim-telab-blasts-nate-silver-about-election-odds-in-series-of-tweets/">[ref]</a>. As a follow-up, if there is general interest, we want to explore a time-series model where the voting intentions follow a random walk. To do this, we need to change the underlying model to allow the unseen random walk influence the polling results. Following Andrew Gelman’s suggestion, we can change the Dirichlet prior to a softmax prior, and then we can make the softmax parameters follow a random walk <a href="http://andrewgelman.com/2009/04/29/conjugate_prior/">[ref]</a></p>

</div>
</div>
</div>
</div>


<!--Footer-->
<footer id="contact" class="page-footer default_color scrollspy">
    <div class="container">
        <div class="row">
            <div class="col l3 s12">
                <h5 class="white-text">datart.com.br</h5>
                <ul>
                    <li class="white-text">contato@datart.com.br</li>
                    <li class="white-text">(19) 98243-9668</li>
                </ul>
            </div>
            <div class="col l3 s12">
                <h5 class="white-text">Social</h5>
                <ul>
                    <li>
                        <a class="white-text" href="https://github.com/datartai">
                            <i class="small fa fa-github-square white-text"></i> Github
                        </a>
                    </li>
                    <li>
                        <a class="white-text" href="https://www.linkedin.com/company/15202895">
                            <i class="small fa fa-linkedin-square white-text"></i> Linkedin
                        </a>
                    </li>
                    <li>
                        <a class="white-text text-lighten-2" href="https://twitter.com/datartai">
                                <i class="small fa fa-twitter-square white-text"></i> Twitter
                            </a>
                    </li>
                </ul>
            </div>
            <div class="col l6 s12">
                <form class="col s12" action="" method="post">
                    <div class="row">
                        <div class="input-field col s8">
                            <i class="mdi-communication-email prefix white-text"></i>
                            <input id="icon_email" name="email" type="email" class="validate white-text">
                            <label for="icon_email" class="white-text">Subscribe to our newsletter</label>
                        </div>
                        <br><div class="col col s4">
                            <button class="btn waves-effect waves-light red darken-1" type="submit">Submit
                                <i class="mdi-content-send right white-text"></i>
                            </button>
                        </div>
                    </div>
                </form>
            </div>
        </div>
    </div>
    <div class="footer-copyright default_color">
        <!-- <div class="container">
            Made by <a class="white-text" href="http://joashpereira.com">Joash Pereira</a>. Thanks to <a class="white-text" href="http://materializecss.com/">materializecss</a>
        </div> -->
    </div>
</footer>


<!--  Scripts-->
<script src="../min/plugin-min.js"></script>
<script src="../min/custom-min.js"></script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-86567228-1', 'auto');
    ga('send', 'pageview');
</script>

</body>
</html>
